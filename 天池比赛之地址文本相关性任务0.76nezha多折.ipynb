{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "civil-nigeria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n",
      "[{'text_id': 'e225b9fd36b8914f42c188fc92e8918f', 'query': '河南省巩义市新华路街道办事处桐和街6号钢苑新区3号楼一单元', 'candidate': [{'text': '巩义市桐和街', 'label': '不匹配'}, {'text': '桐和街依家小店', 'label': '不匹配'}, {'text': '桐和街CHANG六LIULIU', 'label': '不匹配'}, {'text': '桐和街佳乐钢琴', 'label': '不匹配'}, {'text': '世博领秀城南门桐和街囍饭食堂', 'label': '不匹配'}]}, {'text_id': 'b2418ead7b48db4c09caa2934843c1b4', 'query': '老垅坡高家组省建五公司', 'candidate': [{'text': '高家巷省建五公司岳阳分公司', 'label': '完全匹配'}, {'text': '建设北路346号省建5公司', 'label': '不匹配'}, {'text': '老垅坡路西100米省建三公司(岳阳分公司)', 'label': '不匹配'}, {'text': '寇庄西路101号省建五公司', 'label': '不匹配'}, {'text': '卓刀泉南路21号省五建公司省建五公司', 'label': '不匹配'}]}, {'text_id': '5fa94565eb53463fa94ece56e8356fdc', 'query': '西关人和路工商银行对过腾信医药一楼眼镜店', 'candidate': [{'text': '河门口北街33号中国工商银行(河门口支行)', 'label': '不匹配'}, {'text': '河门口北街33号中国工商银行ATM(河门口支行)', 'label': '不匹配'}, {'text': '清香坪东街(食为天旁)中国工商银行24小时自助银行', 'label': '不匹配'}, {'text': '陶家渡东路209号中国工商银行24小时自助银行(巴关河支行)', 'label': '不匹配'}, {'text': '苏铁中路110号中国工商银行24小时自助银行(清香坪支行)', 'label': '不匹配'}]}, {'text_id': '10a2a7c833eea18f479a58f2d15a53b5', 'query': '唐海县四农场场部王玉文', 'candidate': [{'text': '场前路北50米曹妃甸区第四农场', 'label': '部分匹配'}, {'text': '新区曹妃甸湿地曹妃湖东北侧曹妃甸慧钜文化创意产业园', 'label': '不匹配'}, {'text': '建设大街255号四季华庭', 'label': '不匹配'}, {'text': '曹妃甸区西环路', 'label': '不匹配'}, {'text': '华兴路4附近曹妃歌厅(西门)', 'label': '不匹配'}]}, {'text_id': '7b82872ddc84f94733f5dac408d98bce', 'query': '真北路818号近铁城市广场北座二楼', 'candidate': [{'text': '真北路818号近铁城市广场北座', 'label': '部分匹配'}, {'text': '真北路818号近铁城市广场北座(西南2门)', 'label': '部分匹配'}, {'text': '真北路818号近铁城市广场北座(西南1门)', 'label': '部分匹配'}, {'text': '真北路818号近铁城市广场北座2层捞王火锅', 'label': '部分匹配'}, {'text': '金沙江路1685号118广场F1近铁城市广场北座(西北门)', 'label': '部分匹配'}]}, {'text_id': '24dbf73a46d68ef94bd69c8728adeed6', 'query': '义亭工业区甘塘西路9号', 'candidate': [{'text': '义亭镇甘塘西路9号秀颜化妆用具有限公司', 'label': '部分匹配'}, {'text': '义亭镇甘塘西路9-1号义乌市恒凯玩具有限公司', 'label': '部分匹配'}, {'text': '义乌市甘塘西路', 'label': '不匹配'}, {'text': '黄金塘西路9号丹阳英福康电子科技有限公司', 'label': '不匹配'}, {'text': '黄塘西路9号二楼卓悦国际舞蹈学院', 'label': '不匹配'}]}, {'text_id': 'a48b64238490d0447bd5b39e527b280f', 'query': '溧水县永阳镇东山大队谢岗村31号', 'candidate': [{'text': '溧水区谢岗', 'label': '不匹配'}, {'text': '溧水区东山', 'label': '部分匹配'}, {'text': '永阳镇中山大队', 'label': '不匹配'}, {'text': '溧水区东山线', 'label': '不匹配'}, {'text': '东山线附近东山林业队', 'label': '不匹配'}]}, {'text_id': 'adb0c58f99a60ae136daa5148fa188a7', 'query': '纪家庙万兴家居D座二楼', 'candidate': [{'text': '南三环西路玉泉营万兴家居D座2层德高防水', 'label': '部分匹配'}, {'text': '玉泉营桥西万兴家居D座二层德国都芳漆', 'label': '部分匹配'}, {'text': '花乡南三环玉泉营桥万兴家居D管4层羽翔红木', 'label': '部分匹配'}, {'text': '花乡玉泉营桥西万兴家居D座3层69-71五洲装饰', 'label': '部分匹配'}, {'text': '花乡乡南三环西路78号万兴国际家居广场纪家庙地区万兴家居广场', 'label': '部分匹配'}]}, {'text_id': '78f05d3265d15acde1e98cda41cf4f12', 'query': '江苏省南京市江宁区禄口街道欢墩山', 'candidate': [{'text': '江宁区欢墩山', 'label': '部分匹配'}]}, {'text_id': '9601ec765ee5a860992ec83b5743fe42', 'query': '博美二期大门对面莲花新区7号地', 'candidate': [{'text': '围场满族蒙古族自治县七号地', 'label': '不匹配'}, {'text': '金梧桐宾馆东侧100米大屯7号地', 'label': '不匹配'}, {'text': '中原路商隐路交汇处清华·大溪地七号院', 'label': '不匹配'}, {'text': '滨海新区博美园7号楼', 'label': '不匹配'}, {'text': '莲池区秀兰城市美地7号楼', 'label': '不匹配'}]}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "with open('/home/xiaoguzai/数据/天池比赛地址相关数据集/Xeon3NLP_round1_train_20210524.json','r') as load_f:\n",
    "    load_dict = json.load(load_f)\n",
    "    print(load_dict[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-company",
   "metadata": {},
   "source": [
    "读取数据时的注意点：开头要有[]，每一部分中间要有逗号，最后一波中间不要有逗号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "classified-pressing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label[0] question = \n",
      "河南省巩义市新华路街道办事处桐和街6号钢苑新区3号楼一单元\n",
      "label[0] candidate = \n",
      "[{'text': '巩义市桐和街', 'label': '不匹配'}, {'text': '桐和街依家小店', 'label': '不匹配'}, {'text': '桐和街CHANG六LIULIU', 'label': '不匹配'}, {'text': '桐和街佳乐钢琴', 'label': '不匹配'}, {'text': '世博领秀城南门桐和街囍饭食堂', 'label': '不匹配'}]\n"
     ]
    }
   ],
   "source": [
    "print('label[0] question = ')\n",
    "print(load_dict[0]['query'])\n",
    "print('label[0] candidate = ')\n",
    "print(load_dict[0]['candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pregnant-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "nezha_ckpt_dir=\"/home/xiaoguzai/下载/NEZHA-Base-WWM/\"\n",
    "nezha_ckpt_file = nezha_ckpt_dir + \"model.ckpt-691689\"\n",
    "nezha_config_file = nezha_ckpt_dir + \"bert_config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "uniform-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization import FullTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "tokenizer = FullTokenizer(vocab_file=os.path.join(nezha_ckpt_dir,\"vocab.txt\"))\n",
    "question1_id,question2_id = [],[]\n",
    "question1_segment,question2_segment = [],[]\n",
    "question_segment = []\n",
    "segment_id = []\n",
    "question_text = []\n",
    "label_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accurate-turning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin weight0 = \n",
      "0.6069609947834023\n",
      "origin weight1 = \n",
      "0.3307405909378468\n",
      "origin weight2 = \n",
      "0.06229841427875092\n",
      "weight0 = \n",
      "0.07950416546027425\n",
      "weight1 = \n",
      "0.14590264599926467\n",
      "weight2 = \n",
      "0.7745931885404611\n"
     ]
    }
   ],
   "source": [
    "zero_number = 0\n",
    "first_number = 0\n",
    "second_number = 0\n",
    "max_seq_len = 128\n",
    "question_id = []\n",
    "for data in load_dict:\n",
    "    text1 = data['query']\n",
    "    token1 = tokenizer.tokenize(text1)\n",
    "    token1 = [\"[CLS]\"]+token1\n",
    "    token_id1 = tokenizer.convert_tokens_to_ids(token1)\n",
    "    if len(token_id1) > max_seq_len-1:\n",
    "        token_id1 = token_id1[:max_seq_len-1]\n",
    "    token_id1 = token_id1+tokenizer.convert_tokens_to_ids([\"[SEP]\"])\n",
    "    question1_segment = [0]*len(token_id1)\n",
    "    for data1 in data['candidate']:\n",
    "        text2 = data1['text']\n",
    "        token2 = tokenizer.tokenize(text2)\n",
    "        token2 = token2\n",
    "        token_id2 = tokenizer.convert_tokens_to_ids(token2)\n",
    "        if len(token_id2) > max_seq_len-1:\n",
    "            token_id2 = token_id2[:max_seq_len-1]\n",
    "        token_id2 = token_id2+tokenizer.convert_tokens_to_ids([\"[SEP]\"])\n",
    "        question2_segment = [1]*len(token_id2)\n",
    "        token_id = token_id1+token_id2\n",
    "        question_segment = question1_segment+question2_segment\n",
    "        segment_id.append(question_segment)\n",
    "        question_id.append(token_id)\n",
    "        question_text.append(text1+' '+text2)\n",
    "        if data1['label'] == '不匹配':\n",
    "            label_id.append(0)\n",
    "            zero_number = zero_number+1\n",
    "        elif data1['label'] == '部分匹配':\n",
    "            label_id.append(1)\n",
    "            first_number = first_number+1\n",
    "        else:\n",
    "            label_id.append(2)\n",
    "            second_number = second_number+1\n",
    "\n",
    "\n",
    "weight0 = zero_number/(zero_number+first_number+second_number)\n",
    "weight1 = first_number/(zero_number+first_number+second_number)\n",
    "weight2 = second_number/(zero_number+first_number+second_number)\n",
    "print('origin weight0 = ')\n",
    "print(weight0)\n",
    "print('origin weight1 = ')\n",
    "print(weight1)\n",
    "print('origin weight2 = ')\n",
    "print(weight2)\n",
    "weight0 = 1/weight0\n",
    "weight1 = 1/weight1\n",
    "weight2 = 1/weight2\n",
    "totalweight = weight0+weight1+weight2\n",
    "weight0 = weight0/totalweight\n",
    "weight1 = weight1/totalweight\n",
    "weight2 = weight2/totalweight\n",
    "print('weight0 = ')\n",
    "print(weight0)\n",
    "print('weight1 = ')\n",
    "print(weight1)\n",
    "print('weight2 = ')\n",
    "print(weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "informational-lightweight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id = \n",
      "[[101, 3777, 1298, 4689, 2343, 721, 2356, 3173, 1290, 6662, 6125, 6887, 1215, 752, 1905, 3432, 1469, 6125, 127, 1384, 7167, 5723, 3173, 1277, 124, 1384, 3517, 671, 1296, 1039, 102, 2343, 721, 2356, 3432, 1469, 6125, 102], [101, 3777, 1298, 4689, 2343, 721, 2356, 3173, 1290, 6662, 6125, 6887, 1215, 752, 1905, 3432, 1469, 6125, 127, 1384, 7167, 5723, 3173, 1277, 124, 1384, 3517, 671, 1296, 1039, 102, 3432, 1469, 6125, 898, 2157, 2207, 2421, 102], [101, 3777, 1298, 4689, 2343, 721, 2356, 3173, 1290, 6662, 6125, 6887, 1215, 752, 1905, 3432, 1469, 6125, 127, 1384, 7167, 5723, 3173, 1277, 124, 1384, 3517, 671, 1296, 1039, 102, 3432, 1469, 6125, 11680, 1063, 12306, 8636, 8207, 102], [101, 3777, 1298, 4689, 2343, 721, 2356, 3173, 1290, 6662, 6125, 6887, 1215, 752, 1905, 3432, 1469, 6125, 127, 1384, 7167, 5723, 3173, 1277, 124, 1384, 3517, 671, 1296, 1039, 102, 3432, 1469, 6125, 881, 727, 7167, 4433, 102], [101, 3777, 1298, 4689, 2343, 721, 2356, 3173, 1290, 6662, 6125, 6887, 1215, 752, 1905, 3432, 1469, 6125, 127, 1384, 7167, 5723, 3173, 1277, 124, 1384, 3517, 671, 1296, 1039, 102, 686, 1300, 7566, 4899, 1814, 1298, 7305, 3432, 1469, 6125, 1719, 7649, 7608, 1828, 102], [101, 5439, 100, 1786, 7770, 2157, 5299, 4689, 2456, 758, 1062, 1385, 102, 7770, 2157, 2350, 4689, 2456, 758, 1062, 1385, 2277, 7345, 1146, 1062, 1385, 102], [101, 5439, 100, 1786, 7770, 2157, 5299, 4689, 2456, 758, 1062, 1385, 102, 2456, 6392, 1266, 6662, 12380, 1384, 4689, 2456, 126, 1062, 1385, 102], [101, 5439, 100, 1786, 7770, 2157, 5299, 4689, 2456, 758, 1062, 1385, 102, 5439, 100, 1786, 6662, 6205, 8135, 5101, 4689, 2456, 676, 1062, 1385, 113, 2277, 7345, 1146, 1062, 1385, 114, 102], [101, 5439, 100, 1786, 7770, 2157, 5299, 4689, 2456, 758, 1062, 1385, 102, 2167, 2411, 6205, 6662, 8359, 1384, 4689, 2456, 758, 1062, 1385, 102], [101, 5439, 100, 1786, 7770, 2157, 5299, 4689, 2456, 758, 1062, 1385, 102, 1294, 1143, 3787, 1298, 6662, 8128, 1384, 4689, 758, 2456, 1062, 1385, 4689, 2456, 758, 1062, 1385, 102]]\n",
      "label_id = \n",
      "[0, 0, 0, 0, 0, 2, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print('question_id = ')\n",
    "print(question_id[0:10])\n",
    "print('label_id = ')\n",
    "print(label_id[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "included-exposure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config = \n",
      "{'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'max_position_embeddings': 512, 'num_attention_heads': 12, 'num_hidden_layers': 12, 'type_vocab_size': 2, 'vocab_size': 21128, 'use_relative_position': True}\n"
     ]
    }
   ],
   "source": [
    "with open(nezha_config_file,'r') as load_f:\n",
    "    config = json.load(load_f)\n",
    "print('config = ')\n",
    "print(config)\n",
    "config['embedding_size'] = config['hidden_size']\n",
    "config['num_layers'] = config['num_hidden_layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "handy-answer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__ Nezha\n",
      "with call_context.enter\n",
      "with call_context.enter\n",
      "output = \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 768), dtype=tf.float32, name=None), name='lambda/strided_slice:0', description=\"created by layer 'lambda'\")\n",
      "with call_context.enter\n",
      "with call_context.enter\n",
      "with call_context.enter\n",
      "with call_context.enter\n",
      "after Dense\n",
      "output = \n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.float32, name=None), name='dense_1/Softmax:0', description=\"created by layer 'dense_1'\")\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "token_ids1 (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids1 (InputLayer)       [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "nezha (Bert)                    (None, None, 768)    101283840   token_ids1[0][0]                 \n",
      "                                                                 segment_ids1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 768)          0           nezha[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 768)          590592      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            2307        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 101,876,739\n",
      "Trainable params: 101,876,739\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from nezha import Bert\n",
    "batch_size = 48\n",
    "bertmodel = Bert(maxlen=max_seq_len,batch_size=batch_size,**config)\n",
    "input_ids1 = keras.layers.Input(shape=(None,),dtype='int32',name=\"token_ids1\")\n",
    "input_ids2 = keras.layers.Input(shape=(None,),dtype='int32',name=\"segment_ids1\")\n",
    "output = bertmodel([input_ids1,input_ids2])\n",
    "#对应两个输入[token_ids1,segment_ids1]\n",
    "\n",
    "#shape1 = output1.get_shape().as_list()\n",
    "#shape2 = output2.get_shape().as_list()\n",
    "output = keras.layers.Lambda(lambda seq: seq[:,0,:])(output)\n",
    "print('output = ')\n",
    "print(output)\n",
    "output = keras.layers.Dropout(0.2)(output)\n",
    "#!!!这里取出0而不取出-1是因为最后一位可能有填充数值\n",
    "#output1 = tf.reshape(output1,[batch_size,max_seq_len*shape1[2]])\n",
    "#output2 = tf.reshape(output2,[batch_size,max_seq_len*shape1[2]])\n",
    "#原先这里的dropout=0.5,现在的dropout=0.1\n",
    "output = keras.layers.Dense(units=768,activation=\"tanh\")(output)\n",
    "output = keras.layers.Dropout(0.2)(output)\n",
    "output = keras.layers.Dense(units=3,activation=\"softmax\")(output)\n",
    "r\"\"\"\n",
    "class CrossEntropy(tf.keras.layers.Layer):\n",
    "    def call(self,inputs):\n",
    "        loss = K.sparse_categorical_crossentropy(tf.convert_to_tensor(label_id),inputs)\n",
    "        print('label_id = ')\n",
    "        print(tf.convert_to_tensor(label_id))\n",
    "        print('inputs = ')\n",
    "        print(inputs)\n",
    "        print('loss = ')\n",
    "        print(loss)\n",
    "        self.add_loss(loss)\n",
    "        return inputs\n",
    "crossentropy = CrossEntropy()\n",
    "output = crossentropy(output)\n",
    "\"\"\"\n",
    "print('after Dense')\n",
    "print('output = ')\n",
    "print(output)\n",
    "#这里不能使用CrossEntropy的原因在于label_id之中无法取出每一波的批次值，所以选择使用loss函数\n",
    "model = keras.Model(inputs=[input_ids1,input_ids2],outputs=output)\n",
    "#model = keras.Model(inputs=input_ids1,outputs=output)\n",
    "#sparse_categorical_accuracy:检查y_true中的值与y_pred中最大值对应的index是否相等\n",
    "\n",
    "#之前learning_rate = 0.000001,现在learning_rate = 0.00001\n",
    "#from_logits = False输入已经符合某种分布，系统只会把你概率归一化\n",
    "#from_logits = True输入的是原始数据，系统会帮你softmax之后再进行计算\n",
    "#后面指定学习率了，这里的adam就不用指定学习率了\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blocked-document",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_nezha_stock_weights\n",
      "Done loading 196 NEZHA weights from: /home/xiaoguzai/下载/NEZHA-Base-WWM/model.ckpt-691689 into <nezha.Bert object at 0x7fe12c101ee0>. Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbad_steps\n",
      "\tbert/embeddings/LayerNorm/beta/lamb_m\n",
      "\tbert/embeddings/LayerNorm/beta/lamb_v\n",
      "\tbert/embeddings/LayerNorm/gamma/lamb_m\n",
      "\tbert/embeddings/LayerNorm/gamma/lamb_v\n",
      "\tbert/embeddings/token_type_embeddings/lamb_m\n",
      "\tbert/embeddings/token_type_embeddings/lamb_v\n",
      "\tbert/embeddings/word_embeddings/lamb_m\n",
      "\tbert/embeddings/word_embeddings/lamb_v\n",
      "\tbert/encoder/layer_0/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_0/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_0/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_0/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_0/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_0/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_0/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_0/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_0/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_0/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_0/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_0/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_0/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_0/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_0/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_0/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_0/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_0/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_0/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_0/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_0/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_0/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_0/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_0/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_0/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_0/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_0/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_0/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_0/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_0/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_0/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_0/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_1/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_1/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_1/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_1/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_1/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_1/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_1/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_1/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_1/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_1/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_1/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_1/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_1/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_1/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_1/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_1/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_1/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_1/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_1/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_1/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_1/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_1/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_1/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_1/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_1/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_1/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_1/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_1/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_1/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_1/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_1/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_1/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_10/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_10/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_10/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_10/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_10/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_10/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_10/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_10/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_10/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_10/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_10/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_10/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_10/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_10/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_10/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_10/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_10/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_10/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_10/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_10/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_10/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_10/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_10/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_10/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_10/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_10/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_10/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_10/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_10/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_10/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_10/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_10/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_11/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_11/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_11/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_11/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_11/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_11/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_11/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_11/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_11/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_11/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_11/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_11/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_11/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_11/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_11/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_11/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_11/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_11/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_11/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_11/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_11/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_11/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_11/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_11/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_11/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_11/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_11/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_11/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_11/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_11/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_11/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_11/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_2/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_2/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_2/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_2/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_2/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_2/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_2/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_2/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_2/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_2/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_2/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_2/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_2/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_2/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_2/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_2/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_2/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_2/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_2/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_2/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_2/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_2/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_2/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_2/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_2/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_2/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_2/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_2/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_2/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_2/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_2/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_2/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_3/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_3/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_3/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_3/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_3/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_3/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_3/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_3/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_3/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_3/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_3/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_3/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_3/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_3/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_3/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_3/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_3/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_3/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_3/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_3/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_3/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_3/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_3/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_3/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_3/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_3/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_3/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_3/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_3/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_3/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_3/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_3/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_4/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_4/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_4/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_4/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_4/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_4/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_4/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_4/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_4/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_4/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_4/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_4/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_4/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_4/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_4/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_4/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_4/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_4/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_4/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_4/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_4/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_4/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_4/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_4/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_4/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_4/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_4/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_4/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_4/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_4/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_4/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_4/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_5/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_5/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_5/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_5/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_5/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_5/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_5/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_5/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_5/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_5/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_5/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_5/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_5/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_5/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_5/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_5/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_5/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_5/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_5/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_5/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_5/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_5/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_5/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_5/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_5/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_5/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_5/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_5/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_5/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_5/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_5/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_5/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_6/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_6/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_6/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_6/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_6/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_6/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_6/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_6/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_6/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_6/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_6/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_6/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_6/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_6/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_6/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_6/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_6/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_6/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_6/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_6/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_6/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_6/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_6/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_6/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_6/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_6/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_6/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_6/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_6/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_6/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_6/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_6/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_7/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_7/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_7/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_7/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_7/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_7/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_7/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_7/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_7/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_7/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_7/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_7/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_7/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_7/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_7/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_7/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_7/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_7/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_7/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_7/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_7/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_7/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_7/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_7/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_7/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_7/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_7/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_7/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_7/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_7/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_7/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_7/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_8/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_8/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_8/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_8/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_8/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_8/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_8/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_8/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_8/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_8/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_8/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_8/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_8/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_8/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_8/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_8/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_8/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_8/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_8/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_8/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_8/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_8/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_8/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_8/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_8/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_8/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_8/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_8/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_8/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_8/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_8/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_8/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_9/attention/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_9/attention/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_9/attention/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_9/attention/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_9/attention/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_9/attention/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_9/attention/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_9/attention/output/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_9/attention/self/key/bias/lamb_m\n",
      "\tbert/encoder/layer_9/attention/self/key/bias/lamb_v\n",
      "\tbert/encoder/layer_9/attention/self/key/kernel/lamb_m\n",
      "\tbert/encoder/layer_9/attention/self/key/kernel/lamb_v\n",
      "\tbert/encoder/layer_9/attention/self/query/bias/lamb_m\n",
      "\tbert/encoder/layer_9/attention/self/query/bias/lamb_v\n",
      "\tbert/encoder/layer_9/attention/self/query/kernel/lamb_m\n",
      "\tbert/encoder/layer_9/attention/self/query/kernel/lamb_v\n",
      "\tbert/encoder/layer_9/attention/self/value/bias/lamb_m\n",
      "\tbert/encoder/layer_9/attention/self/value/bias/lamb_v\n",
      "\tbert/encoder/layer_9/attention/self/value/kernel/lamb_m\n",
      "\tbert/encoder/layer_9/attention/self/value/kernel/lamb_v\n",
      "\tbert/encoder/layer_9/intermediate/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_9/intermediate/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_9/intermediate/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_9/intermediate/dense/kernel/lamb_v\n",
      "\tbert/encoder/layer_9/output/LayerNorm/beta/lamb_m\n",
      "\tbert/encoder/layer_9/output/LayerNorm/beta/lamb_v\n",
      "\tbert/encoder/layer_9/output/LayerNorm/gamma/lamb_m\n",
      "\tbert/encoder/layer_9/output/LayerNorm/gamma/lamb_v\n",
      "\tbert/encoder/layer_9/output/dense/bias/lamb_m\n",
      "\tbert/encoder/layer_9/output/dense/bias/lamb_v\n",
      "\tbert/encoder/layer_9/output/dense/kernel/lamb_m\n",
      "\tbert/encoder/layer_9/output/dense/kernel/lamb_v\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/bias/lamb_m\n",
      "\tbert/pooler/dense/bias/lamb_v\n",
      "\tbert/pooler/dense/kernel\n",
      "\tbert/pooler/dense/kernel/lamb_m\n",
      "\tbert/pooler/dense/kernel/lamb_v\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/output_bias/lamb_m\n",
      "\tcls/predictions/output_bias/lamb_v\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/beta/lamb_m\n",
      "\tcls/predictions/transform/LayerNorm/beta/lamb_v\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/LayerNorm/gamma/lamb_m\n",
      "\tcls/predictions/transform/LayerNorm/gamma/lamb_v\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/bias/lamb_m\n",
      "\tcls/predictions/transform/dense/bias/lamb_v\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/predictions/transform/dense/kernel/lamb_m\n",
      "\tcls/predictions/transform/dense/kernel/lamb_v\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_bias/lamb_m\n",
      "\tcls/seq_relationship/output_bias/lamb_v\n",
      "\tcls/seq_relationship/output_weights\n",
      "\tcls/seq_relationship/output_weights/lamb_m\n",
      "\tcls/seq_relationship/output_weights/lamb_v\n",
      "\tglobal_step\n",
      "\tgood_steps\n",
      "\tloss_scale\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nezha_loader import load_nezha_stock_weights\n",
    "load_nezha_stock_weights(nezha=bertmodel,ckpt_path=nezha_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "statewide-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "result_point = []\n",
    "test_point = []\n",
    "#kf = KFold(n_splits=5,shuffle=True)\n",
    "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
    "                                   end_learn_rate=1e-7,\n",
    "                                   warmup_epoch_count=10,\n",
    "                                   total_epoch_count=90):\n",
    "    print('create_learning_rate_scheduler')\n",
    "    def lr_scheduler(epoch):\n",
    "    #18年Facebook提出的gradual warmup刚开始训练的时候，模型的权重是随机\n",
    "    #初始化的，此时若选择一个较大的学习率，可能带来模型的不确定(振荡),选择warm\n",
    "    #up学习率的方式，可以使得开始训练的几个epoches或者一些steps学习率较小\n",
    "    #在预热的小学习率下，模型可以慢慢趋于稳定，等模型相对稳定后再选择预先设置学习率\n",
    "        if epoch < warmup_epoch_count:\n",
    "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
    "        else:\n",
    "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
    "        return float(res)\n",
    "    #Warmup不足之处在于从一个很小的学习率一下子变为比较大的学习率可能会导致训练误差突然增大\n",
    "    #facebook提出了从最初的小学习率开始，每个step增大一点\n",
    "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "    #keras.callbacks.LearningRateScheduler(schedule,verbose=0)\n",
    "    #schedule:接受epoch作为输入(整数，从0开始迭代)，然后返回一个学习率作为输出(浮点数)\n",
    "    #verbose:0:安静,1:更新信息\n",
    "    return learning_rate_scheduler\n",
    "    \n",
    "#split = StratifiedShuffleSplit(n_splits=1,test_size=0.1,random_state=42)\n",
    "#split = StratifiedShuffleSplit(n_splits=5,test_size=0.1,random_state=42)\n",
    "#分成5组，每组中的test=0.1也就是说循环之中使用的for循环每次从5组之中取出一组数据\n",
    "#所以之前一直在用一组数据进行训练\n",
    "class DataGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self,token_ids,segment_ids,label_ids,batch_size=48,max_seq_len=128):\n",
    "        self.token_ids = token_ids\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        self.batch_size = batch_size\n",
    "        self.maxlen = max_seq_len\n",
    "        self.totals = len(self.token_ids)\n",
    "        self.best_score = 0\n",
    "        self.train_token_ids = None\n",
    "        self.test_token_ids = None\n",
    "        self.train_segment_ids = None\n",
    "        self.test_segment_ids = None\n",
    "        self.train_label = None\n",
    "        self.test_label = None\n",
    "        #这个取出数值放在__init__函数之中，每次都是一样的数值，如果放在cycle\n",
    "        #函数之中，每次取出来的数值都是不一样的\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.token_ids)/self.batch_size))\n",
    "\n",
    "    def sample(self,random=False):\n",
    "        #sample拆分出训练集进行训练\n",
    "        indices = list(range(len(self.train_token_ids)))\n",
    "        np.random.shuffle(indices)\n",
    "        for i in indices:\n",
    "            yield self.train_token_ids[i],self.train_segment_ids[i],self.train_label[i]\n",
    "    \n",
    "    def __iter__(self,random=False):\n",
    "        #__iter__拆分出对应的batch\n",
    "        random = False\n",
    "        batch_token_ids = []\n",
    "        batch_segment_ids = []\n",
    "        batch_label_ids = []\n",
    "        currents = 0\n",
    "        batch_data = []\n",
    "        for token_ids,segment_ids,label_ids in self.sample(random):\n",
    "            if len(token_ids) > self.maxlen*2:\n",
    "            #!!!注意这里要为self.maxlen*2,因为单个长度为128，两个加起来不能超过256\n",
    "                token_ids = token_ids[:self.maxlen]\n",
    "                segment_ids = segment_ids[:self.maxlen]\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_label_ids.append(label_ids)\n",
    "            \n",
    "            r\"\"\"\n",
    "            k折交叉验证的时候就不加量了\n",
    "            if label_ids == 2:\n",
    "            #!!!标记部分\n",
    "                batch_token_ids.append(token_ids)\n",
    "                batch_segment_ids.append(segment_ids)\n",
    "                batch_label_ids.append(label_ids)\n",
    "                batch_token_ids.append(token_ids)\n",
    "                batch_segment_ids.append(segment_ids)\n",
    "                batch_label_ids.append(label_ids)\n",
    "            \"\"\"\n",
    "            #实验表明只加两波效果比较好\n",
    "            currents = currents+1\n",
    "            if len(batch_token_ids) == self.batch_size or currents == self.totals:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                yield [np.array(batch_token_ids),np.array(batch_segment_ids)],np.array(batch_label_ids)\n",
    "                #yield [np.array(batch_token_ids1),np.array(batch_segment_ids1)],np.array(batch_label_ids)\n",
    "                batch_token_ids,batch_segment_ids,batch_label_ids = [],[],[]\n",
    "                #这里必须要统一维度\n",
    "                \n",
    "    def cycle(self,random=True):\n",
    "        #for index1,index2 in kf.split(self.token_ids1,self.token_ids1):\n",
    "        flag = False\n",
    "        split = StratifiedShuffleSplit(n_splits=5,test_size=0.1)\n",
    "        #!!!这个split一定要定义在cycle之中，否则定义好了的random_state对应值一样\n",
    "        for index1,index2 in split.split(self.token_ids,self.label_ids):\n",
    "            self.train_token_ids = np.array(self.token_ids)[index1]\n",
    "            self.test_token_ids = np.array(self.token_ids)[index2]\n",
    "            self.train_segment_ids = np.array(self.segment_ids)[index1]\n",
    "            self.test_segment_ids = np.array(self.segment_ids)[index2]\n",
    "            self.train_label = np.array(self.label_ids)[index1]\n",
    "            self.test_label = np.array(self.label_ids)[index2]\n",
    "        while True:\n",
    "            for d in self.__iter__(random):\n",
    "                yield d\n",
    "    \n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        zero_reality = 0\n",
    "        partial_reality = 0\n",
    "        true_reality = 0\n",
    "        zero_predict = 0\n",
    "        partial_predict = 0\n",
    "        true_predict = 0\n",
    "        zero_true = 0\n",
    "        partial_true = 0\n",
    "        true_true = 0\n",
    "        predict_label = []\n",
    "        data2 = []\n",
    "        for i in tqdm(range(0,len(self.test_token_ids))):\n",
    "            token_id = self.test_token_ids[i]\n",
    "            segment_id = self.test_segment_ids[i]\n",
    "            res = model.predict([np.array([token_id]),np.array([segment_id])]).argmax(axis=-1)\n",
    "            data2.append(res[0])\n",
    "            if res[0] == 0 and self.test_label[i] == 0:\n",
    "                zero_true = zero_true+1\n",
    "            elif res[0] == 1 and self.test_label[i] == 1:\n",
    "                partial_true = partial_true+1\n",
    "            elif res[0] == 2 and self.test_label[i] == 2:\n",
    "                true_true = true_true+1\n",
    "            predict_label.append(res[0])\n",
    "        sets = Counter(self.test_label)\n",
    "        zero_reality = sets[0]\n",
    "        partial_reality = sets[1]\n",
    "        true_reality = sets[2]\n",
    "        zero_predict = predict_label.count(0)\n",
    "        partial_predict = predict_label.count(1)\n",
    "        true_predict = predict_label.count(2)\n",
    "        \n",
    "        if zero_predict == 0:\n",
    "            zero_point1 = 0\n",
    "        else:\n",
    "            zero_point1 = zero_true/zero_predict   \n",
    "\n",
    "        if zero_reality == 0:\n",
    "            zero_point2 = 0\n",
    "        else:\n",
    "            zero_point2 = zero_true/zero_reality\n",
    "            \n",
    "        if zero_point1+zero_point2 == 0:\n",
    "            zero_point3 = 0\n",
    "        else:\n",
    "            zero_point3 = 2*zero_point1*zero_point2/(zero_point1+zero_point2)\n",
    "        \n",
    "        if partial_predict == 0:\n",
    "            partial_point1 = 0\n",
    "        else:\n",
    "            partial_point1 = partial_true/partial_predict\n",
    "\n",
    "        if partial_reality == 0:\n",
    "            partial_point2 = 0\n",
    "        else:\n",
    "            partial_point2 = partial_true/partial_reality\n",
    "\n",
    "        if partial_point1+partial_point2 == 0:\n",
    "            partial_point3 = 0\n",
    "        else:\n",
    "            partial_point3 = 2*partial_point1*partial_point2/(partial_point1+partial_point2)\n",
    "        \n",
    "        if true_predict == 0:\n",
    "            true_point1 = 0\n",
    "        else:\n",
    "            true_point1 = true_true/true_predict\n",
    "\n",
    "        if true_reality == 0:\n",
    "            true_point2 = 0\n",
    "        else:\n",
    "            true_point2 = true_true/true_reality\n",
    "\n",
    "        if true_point1+true_point2 == 0:\n",
    "            true_point3 = 0\n",
    "        else:\n",
    "            true_point3 = 2*true_point1*true_point2/(true_point1+true_point2)\n",
    "        \n",
    "        total_score1 = (zero_point1+zero_point2+zero_point3)/3\n",
    "        total_score2 = (partial_point1+partial_point2+partial_point3)/3\n",
    "        total_score3 = (true_point1+true_point2+true_point3)/3\n",
    "        final_score = total_score1+total_score2+total_score3\n",
    "        final_score = final_score/3\n",
    "        print('验证集')\n",
    "        print('分类1 准确率')\n",
    "        print(zero_point1)\n",
    "        print('分类1 召回率')\n",
    "        print(zero_point2)\n",
    "        print('分类1 f1_score')\n",
    "        print(zero_point3)\n",
    "        print('分类2 准确率')\n",
    "        print(partial_point1)\n",
    "        print('分类2 召回率')\n",
    "        print(partial_point2)\n",
    "        print('分类2 f1_score')\n",
    "        print(partial_point3)\n",
    "        print('分类3 准确率')\n",
    "        print(true_point1)\n",
    "        print('分类3 召回率')\n",
    "        print(true_point2)\n",
    "        print('分类3 f1_score')\n",
    "        print(true_point3)\n",
    "        print('current final_score = ')\n",
    "        print(final_score)\n",
    "        result_point.append(final_score)\n",
    "        data1 = self.test_label\n",
    "        data1 = np.expand_dims(data1,axis=-1)\n",
    "        data2 = predict_label\n",
    "        data2 = np.expand_dims(data2,axis=-1)\n",
    "        point = keras.metrics.sparse_categorical_accuracy(data1,data2)\n",
    "        print('evaluate sparse_categorical_accuracy = ')\n",
    "        print(sum(point)/len(predict_label))\n",
    "        print('#######################################')\n",
    "        if final_score > self.best_score:\n",
    "            self.best_score = final_score\n",
    "            print('self.best_score = ')\n",
    "            print(self.best_score)\n",
    "            model.save_weights('./天池地址匹配the_best_model.h5')\n",
    "            #注意这里存的为epoch+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "capital-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padding(inputs,padding = 0):\n",
    "    length = max([len(x) for x in inputs])\n",
    "    pad_width = [(0,0) for _ in np.shape(inputs[0])]\n",
    "    outputs = []\n",
    "    for x in inputs:\n",
    "        x = x[:length]\n",
    "        pad_width[0] = (0,length-len(x))\n",
    "        x = np.pad(x,pad_width,'constant',constant_values=padding)\n",
    "        outputs.append(x)\n",
    "    return np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "closing-intermediate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with call_context.enter\n"
     ]
    }
   ],
   "source": [
    "#output_ids = model(input_ids1)\n",
    "output_ids = model([input_ids1,input_ids2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "interested-saturn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(question1_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-staff",
   "metadata": {},
   "source": [
    "提前停止模型的使用方法：\n",
    "```python\n",
    "import keras\n",
    "early_stopping=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "                              patience=0, verbose=0, mode='auto',\n",
    "                              baseline=None, restore_best_weights=False)\n",
    "model.fit(callbacks = [early_stopping])\n",
    "```\n",
    "这里的monitor可以换成为train_generator.best_score\n",
    "如果best_score没有提升，就停止模型的寻俩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "photographic-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_true,y_pred):\n",
    "    print('y_true = ')\n",
    "    print(y_true)\n",
    "    print('y_pred = ')\n",
    "    print(y_pred)\n",
    "    loss = K.sparse_categorical_crossentropy(y_true,y_pred)\n",
    "    print('loss = ')\n",
    "    print(loss)\n",
    "    #result = np.sum(loss,axis=-1)/3\n",
    "    #return loss[0]+2*loss[1]+5*loss[2]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "seeing-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 2, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_id[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "enhanced-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(question_id,segment_id,label_id)\n",
    "model.compile(#optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              #loss=compute_loss,\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-bishop",
   "metadata": {},
   "source": [
    "train_generator.on_epoch_end(epoch=1,logs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-chapter",
   "metadata": {},
   "source": [
    "model.fit(\n",
    "    train_generator.cycle(),\n",
    "    steps_per_epoch = len(train_generator),\n",
    "    epochs = 10,\n",
    "    callbacks = [train_generator]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "arctic-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!!!!!!!!!两处需要改正的一处在create_learning_rate_schduler,一处在model.save_weights那个地方!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-wildlife",
   "metadata": {},
   "source": [
    "model.load_weights('./天池地址匹配the_best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cutting-nudist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-4c972a307f22>:106: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.train_token_ids = np.array(self.token_ids)[index1]\n",
      "<ipython-input-10-4c972a307f22>:107: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.test_token_ids = np.array(self.token_ids)[index2]\n",
      "<ipython-input-10-4c972a307f22>:108: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.train_segment_ids = np.array(self.segment_ids)[index1]\n",
      "<ipython-input-10-4c972a307f22>:109: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  self.test_segment_ids = np.array(self.segment_ids)[index2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  26/2008 [..............................] - ETA: 9:09 - loss: 0.8824 - acc: 0.5419"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fae8405aba09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 _r=1):\n\u001b[1;32m   1186\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    898\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator.cycle(),\n",
    "    steps_per_epoch = len(train_generator),\n",
    "    epochs = 20,\n",
    "    callbacks = [train_generator]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('/home/xiaoguzai/数据/天池比赛地址相关数据集/验证集错误部分.txt','w')\n",
    "current_str = ['不匹配','部分匹配','完全匹配']\n",
    "for i in tqdm(range(0,len(train_generator.test_token_ids))):\n",
    "    token_id = train_generator.test_token_ids[i]\n",
    "    segment_id = train_generator.test_segment_ids[i]\n",
    "    res = model.predict([np.array([token_id]),np.array([segment_id])]).argmax(axis=-1)\n",
    "    if res[0] != train_generator.test_label[i]:\n",
    "        fp.write(str(i)+' '+question_text[i]+'\\n')\n",
    "        fp.write('my_label:'+current_str[res[0]])\n",
    "        fp.write(' true_label:'+current_str[train_generator.test_label[i]]+'\\n')\n",
    "fp.close()\n",
    "r\"\"\"\n",
    "data2.append(res[0])\n",
    "if res[0] == 0 and train_generator.test_label[i] == 0:\n",
    "    zero_true = zero_true+1\n",
    "elif res[0] == 1 and train_generator.test_label[i] == 1:\n",
    "    partial_true = partial_true+1\n",
    "elif res[0] == 2 and train_generator.test_label[i] == 2:\n",
    "    true_true = true_true+1\n",
    "predict_label.append(res[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-combine",
   "metadata": {},
   "source": [
    "model.load_weights('./天池地址匹配the_best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/xiaoguzai/数据/天池比赛地址相关数据集/Xeon3NLP_round1_test_20210524.json','r') as load_f:\n",
    "    test_data = json.load(load_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-refund",
   "metadata": {},
   "source": [
    "model.save_weights('./天池地址匹配epoch=3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data in test_data:\n",
    "for i in tqdm(range(0,len(test_data))):\n",
    "    data = test_data[i]\n",
    "    text1 = data['query']\n",
    "    token1 = tokenizer.tokenize(text1)\n",
    "    token1 = [\"[CLS]\"]+token1\n",
    "    token_id1 = tokenizer.convert_tokens_to_ids(token1)\n",
    "    if len(token_id1) > max_seq_len-1:\n",
    "        token_id1 = token_id1[:max_seq_len-1]\n",
    "    token_id1 = token_id1+tokenizer.convert_tokens_to_ids([\"[SEP]\"])\n",
    "    segment_id1 = [0]*len(token_id1)\n",
    "    for j in range(len(data['candidate'])):\n",
    "        data1 = data['candidate'][j]\n",
    "        text2 = data1['text']\n",
    "        token2 = tokenizer.tokenize(text2)\n",
    "        token_id2 = tokenizer.convert_tokens_to_ids(token2)\n",
    "        if len(token_id2) > max_seq_len-1:\n",
    "            token_id2 = token_id2[:max_seq_len-1]\n",
    "        token_id2 = token_id2+tokenizer.convert_tokens_to_ids([\"[SEP]\"])\n",
    "        segment_id2 = [1]*len(token_id2)\n",
    "        current = [token_id1+token_id2]\n",
    "        res = model.predict([np.array([token_id1+token_id2]),np.array([segment_id1+segment_id2])]).argmax(axis=-1)\n",
    "        if res[0] == 0:\n",
    "            test_data[i]['candidate'][j]['label'] = '不匹配'\n",
    "        elif res[0] == 1:\n",
    "            test_data[i]['candidate'][j]['label'] = '部分匹配'\n",
    "        else:\n",
    "            test_data[i]['candidate'][j]['label'] = '完全匹配'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_obj = open('/home/xiaoguzai/数据/天池比赛地址相关数据集/results3.txt','w',encoding='utf-8')\n",
    "file_obj = open('nezha_提交结果_results.txt','w',encoding='utf-8')\n",
    "for data in test_data:\n",
    "    file_obj.write(json.dumps(data,ensure_ascii=False))\n",
    "    file_obj.write('\\n')\n",
    "file_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj = open('nezha计算结果1.txt','w',encoding='utf-8')\n",
    "for index in range(len(test_data)):\n",
    "    file_obj.write(str(index)+' '+str(test_data[i]['candidate'][j]['label'])+'\\n')\n",
    "file_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
